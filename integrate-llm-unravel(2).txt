#!/bin/bash
# Full automatic integration of LLM capabilities into Unravel AI
set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
REPO_DIR="$(cd "${SCRIPT_DIR}" && pwd)"
PYTHON_CMD="$(command -v python3 || command -v python)"
FORCE_INSTALL=0
SKIP_DEPS=0

# Process arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --force) FORCE_INSTALL=1; shift ;;
        --skip-deps) SKIP_DEPS=1; shift ;;
        *) echo "Unknown option: $1"; exit 1 ;;
    esac
done

# Create needed directories
mkdir -p "${REPO_DIR}/src/core"
mkdir -p "${REPO_DIR}/scripts"
mkdir -p "${HOME}/.cache/unravel-ai/models"
mkdir -p "${HOME}/.config/unravel-ai"

# Install Python dependencies if needed
if [ $SKIP_DEPS -eq 0 ]; then
    echo "Installing required Python packages..."
    ${PYTHON_CMD} -m pip install --upgrade pip
    ${PYTHON_CMD} -m pip install huggingface_hub requests numpy psutil setuptools wheel
    ${PYTHON_CMD} -m pip install llama-cpp-python
fi

# Create the LLM interface module
cat > "${REPO_DIR}/src/core/llm_interface.py" << 'EOF'
#!/usr/bin/env python3
"""
Unravel AI - Comprehensive LLM Integration with llama.cpp for CPU inference
"""
import os
import sys
import re
import json
import time
import asyncio
import logging
import hashlib
import tempfile
from typing import Dict, List, Any, Optional, Union, Tuple, Set, Callable
from pathlib import Path
import subprocess
import psutil
import concurrent.futures
import requests
import numpy as np
from enum import Enum
import importlib.util
import shutil

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)

# Auto-detect availability of optional dependencies
HAS_TRANSFORMERS = importlib.util.find_spec("transformers") is not None
HAS_HUGGINGFACE_HUB = importlib.util.find_spec("huggingface_hub") is not None
HAS_TOKENIZERS = importlib.util.find_spec("tokenizers") is not None
HAS_CTRANSFORMERS = importlib.util.find_spec("ctransformers") is not None
HAS_LLAMA_CPP = importlib.util.find_spec("llama_cpp") is not None

class LLMProvider(str, Enum):
    """Supported LLM providers"""
    LLAMACPP_API = "llamacpp_api"
    LLAMACPP_PYTHON = "llamacpp_python"
    CTRANSFORMERS = "ctransformers"
    HUGGINGFACE = "huggingface"
    OLLAMA = "ollama"
    LOCAL_API = "local_api"

class AnalysisTask(str, Enum):
    """Types of code analysis tasks"""
    SPECIFICATION = "specification"
    DOCUMENTATION = "documentation"
    VULNERABILITY = "vulnerability"
    REFACTORING = "refactoring"
    FUNCTION_SUMMARY = "function_summary"
    CLASS_SUMMARY = "class_summary"
    ALGORITHM_DETECTION = "algorithm_detection"
    COMPLEXITY_ANALYSIS = "complexity_analysis"
    CUSTOM = "custom"

class LLMMessage:
    """Represents a message in a conversation with an LLM"""
    
    def __init__(self, role: str, content: str):
        """
        Initialize a message
        
        Args:
            role: Message role ("system", "user", "assistant")
            content: Message content
        """
        self.role = role
        self.content = content
    
    def to_dict(self) -> Dict[str, str]:
        """Convert to dictionary for API requests"""
        return {"role": self.role, "content": self.content}

class LLMRequestOptions:
    """Options for an LLM request"""
    
    def __init__(self, 
                 temperature: float = 0.2,
                 top_p: float = 0.95,
                 top_k: int = 40,
                 max_tokens: int = 4096,
                 stream: bool = False,
                 stop_sequences: Optional[List[str]] = None,
                 repetition_penalty: float = 1.1,
                 num_threads: Optional[int] = None):
        """
        Initialize request options
        
        Args:
            temperature: Temperature for sampling
            top_p: Top-p probability threshold
            top_k: Top-k sampling threshold
            max_tokens: Maximum tokens to generate
            stream: Whether to stream the response
            stop_sequences: Sequences to stop generation
            repetition_penalty: Penalty for token repetition
            num_threads: Number of CPU threads to use
        """
        self.temperature = temperature
        self.top_p = top_p
        self.top_k = top_k
        self.max_tokens = max_tokens
        self.stream = stream
        self.stop_sequences = stop_sequences or []
        self.repetition_penalty = repetition_penalty
        self.num_threads = num_threads or max(1, psutil.cpu_count(logical=False) - 1)

class LLMUsage:
    """Tracks token usage"""
    
    def __init__(self, prompt_tokens: int = 0, completion_tokens: int = 0):
        """
        Initialize token usage
        
        Args:
            prompt_tokens: Number of tokens in the prompt
            completion_tokens: Number of tokens in the completion
        """
        self.prompt_tokens = prompt_tokens
        self.completion_tokens = completion_tokens
        self.total_tokens = prompt_tokens + completion_tokens

class LLMResponse:
    """Response from an LLM"""
    
    def __init__(self, content: str, usage: LLMUsage, model_id: str, finish_reason: str = "stop"):
        """
        Initialize a response
        
        Args:
            content: Response content
            usage: Token usage statistics
            model_id: ID of the model used
            finish_reason: Reason for finishing generation
        """
        self.content = content
        self.usage = usage
        self.model_id = model_id
        self.finish_reason = finish_reason

class TokenCounter:
    """Approximate token counter for LLMs"""
    
    def __init__(self):
        """Initialize the token counter"""
        self.words_per_token = 0.75  # Approximate ratio
    
    def count_tokens(self, text: str) -> int:
        """
        Count tokens in a text string (approximate)
        
        Args:
            text: Text to count tokens for
            
        Returns:
            Approximate token count
        """
        # Simple approximation based on word count
        words = len(text.split())
        return int(words / self.words_per_token)

class PromptBuilder:
    """Builds effective prompts for different LLM models"""
    
    def __init__(self, model_id: str):
        """
        Initialize the prompt builder
        
        Args:
            model_id: Model ID or name
        """
        self.model_id = model_id.lower()
        self.token_counter = TokenCounter()
    
    def build_messages(self, system_prompt: str, user_prompt: str) -> List[LLMMessage]:
        """
        Build messages for a conversation
        
        Args:
            system_prompt: System instruction
            user_prompt: User query or input
            
        Returns:
            List of messages
        """
        return [
            LLMMessage("system", system_prompt),
            LLMMessage("user", user_prompt)
        ]
    
    def format_prompt(self, messages: List[LLMMessage]) -> str:
        """
        Format messages into a prompt string based on the model
        
        Args:
            messages: List of messages
            
        Returns:
            Formatted prompt
        """
        # Llama 2 Chat format
        if "llama-2" in self.model_id and "chat" in self.model_id:
            return self._format_llama2_chat(messages)
        # Mistral format
        elif "mistral" in self.model_id:
            return self._format_mistral(messages)
        # CodeLlama format
        elif "codellama" in self.model_id:
            return self._format_codellama(messages)
        # Phi format
        elif "phi" in self.model_id:
            return self._format_phi(messages)
        # Default to simple format
        else:
            return self._format_simple(messages)
    
    def _format_llama2_chat(self, messages: List[LLMMessage]) -> str:
        """Format messages for Llama 2 Chat models"""
        system_prompt = "You are a helpful, respectful and honest assistant."
        formatted_messages = []
        
        # Extract system prompt if present
        for msg in messages:
            if msg.role == "system":
                system_prompt = msg.content
                break
        
        # Build conversation with Llama 2 format
        conversation = f"<s>[INST] <<SYS>>\n{system_prompt}\n<</SYS>>\n\n"
        
        for i, msg in enumerate(messages):
            if msg.role == "system":
                continue
            elif msg.role == "user":
                if i > 0 and conversation and not conversation.endswith("[INST] "):
                    conversation += " [/INST] "
                if i > 0 and not conversation.endswith("[INST] "):
                    conversation += "[INST] "
                conversation += msg.content
            elif msg.role == "assistant":
                conversation += f" [/INST] {msg.content} </s>"
                if i < len(messages) - 1:
                    conversation += "<s>"
        
        # Close the final user message if needed
        if conversation and not conversation.endswith("[/INST] ") and not conversation.endswith("</s>"):
            conversation += " [/INST] "
        
        return conversation
    
    def _format_mistral(self, messages: List[LLMMessage]) -> str:
        """Format messages for Mistral models"""
        formatted = ""
        system_content = None
        
        # Extract system message if present
        for msg in messages:
            if msg.role == "system":
                system_content = msg.content
                break
        
        # Start with the system message if present
        if system_content:
            formatted += f"<s>[INST] {system_content} [/INST]</s>"
        
        # Add messages
        for i, msg in enumerate(messages):
            if msg.role == "system":
                continue
            elif msg.role == "user":
                formatted += f"<s>[INST] {msg.content} [/INST]"
            elif msg.role == "assistant":
                formatted += f" {msg.content}</s>"
        
        return formatted
    
    def _format_codellama(self, messages: List[LLMMessage]) -> str:
        """Format messages for CodeLlama models"""
        # CodeLlama uses the same format as Llama 2 Chat
        return self._format_llama2_chat(messages)
    
    def _format_phi(self, messages: List[LLMMessage]) -> str:
        """Format messages for Phi models"""
        formatted = ""
        
        # Process messages
        for msg in messages:
            if msg.role == "system":
                formatted += f"System: {msg.content}\n\n"
            elif msg.role == "user":
                formatted += f"Human: {msg.content}\n\n"
            elif msg.role == "assistant":
                formatted += f"Assistant: {msg.content}\n\n"
        
        # Add assistant prefix for generation
        if messages[-1].role != "assistant":
            formatted += "Assistant: "
        
        return formatted
    
    def _format_simple(self, messages: List[LLMMessage]) -> str:
        """Simple format for any model"""
        formatted = ""
        
        # Extract system message
        system_msg = next((msg for msg in messages if msg.role == "system"), None)
        if system_msg:
            formatted += f"### System:\n{system_msg.content}\n\n"
        
        # Add user and assistant messages
        for msg in messages:
            if msg.role == "system":
                continue
            elif msg.role == "user":
                formatted += f"### User:\n{msg.content}\n\n"
            elif msg.role == "assistant":
                formatted += f"### Assistant:\n{msg.content}\n\n"
        
        # Add assistant prompt for generation
        if messages[-1].role != "assistant":
            formatted += "### Assistant:\n"
        
        return formatted

class LLMAPIClient:
    """Client for interacting with LLM APIs"""
    
    def __init__(self, 
                 provider: LLMProvider,
                 model_id: str,
                 api_base: Optional[str] = None,
                 timeout: int = 120):
        """
        Initialize the API client
        
        Args:
            provider: LLM provider
            model_id: Model ID or name
            api_base: Base URL for API
            timeout: Request timeout in seconds
        """
        self.provider = provider
        self.model_id = model_id
        self.timeout = timeout
        
        # Set API base URL
        if api_base:
            self.api_base = api_base
        elif provider == LLMProvider.LLAMACPP_API:
            self.api_base = "http://localhost:8080"
        elif provider == LLMProvider.OLLAMA:
            self.api_base = "http://localhost:11434/api"
        elif provider == LLMProvider.LOCAL_API:
            self.api_base = "http://localhost:8000"
        else:
            self.api_base = None
        
        self.prompt_builder = PromptBuilder(model_id)
        self.token_counter = TokenCounter()
    
    def generate(self, messages: List[LLMMessage], options: LLMRequestOptions) -> LLMResponse:
        """
        Generate a response using the API
        
        Args:
            messages: List of messages
            options: Generation options
            
        Returns:
            LLM response
        """
        # Handle each provider type
        if self.provider == LLMProvider.LLAMACPP_API:
            return self._generate_llamacpp_api(messages, options)
        elif self.provider == LLMProvider.OLLAMA:
            return self._generate_ollama(messages, options)
        elif self.provider == LLMProvider.LOCAL_API:
            return self._generate_local_api(messages, options)
        else:
            raise ValueError(f"Unsupported provider for API client: {self.provider}")
    
    def _generate_llamacpp_api(self, messages: List[LLMMessage], options: LLMRequestOptions) -> LLMResponse:
        """Generate using llama.cpp HTTP server"""
        # Format prompt
        prompt = self.prompt_builder.format_prompt(messages)
        
        # Count tokens
        prompt_tokens = self.token_counter.count_tokens(prompt)
        
        # Prepare request
        request_data = {
            "prompt": prompt,
            "n_predict": options.max_tokens,
            "temperature": options.temperature,
            "top_p": options.top_p,
            "top_k": options.top_k,
            "repeat_penalty": options.repetition_penalty,
            "stop": options.stop_sequences if options.stop_sequences else None,
            "stream": options.stream,
            "n_threads": options.num_threads
        }
        
        # Make request
        try:
            if options.stream:
                full_response = ""
                response = requests.post(
                    f"{self.api_base}/completion",
                    json=request_data,
                    stream=True,
                    timeout=self.timeout
                )
                response.raise_for_status()
                
                for line in response.iter_lines():
                    if not line:
                        continue
                    
                    chunk = json.loads(line)
                    content = chunk.get("content", "")
                    full_response += content
                    
                    # Check if generation is done
                    if chunk.get("stop", False):
                        break
            else:
                response = requests.post(
                    f"{self.api_base}/completion",
                    json=request_data,
                    timeout=self.timeout
                )
                response.raise_for_status()
                
                data = response.json()
                full_response = data.get("content", "")
        
        except Exception as e:
            logger.error(f"Error calling llama.cpp API: {str(e)}")
            raise RuntimeError(f"LLM API error: {str(e)}")
        
        # Strip any prefix matching the prompt
        if full_response.startswith(prompt):
            full_response = full_response[len(prompt):]
        
        # Count completion tokens
        completion_tokens = self.token_counter.count_tokens(full_response)
        
        # Create usage metrics
        usage = LLMUsage(
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens
        )
        
        return LLMResponse(
            content=full_response.strip(),
            usage=usage,
            model_id=self.model_id
        )
    
    def _generate_ollama(self, messages: List[LLMMessage], options: LLMRequestOptions) -> LLMResponse:
        """Generate using Ollama API"""
        # Prepare request data
        formatted_messages = [msg.to_dict() for msg in messages]
        
        request_data = {
            "model": self.model_id,
            "messages": formatted_messages,
            "options": {
                "temperature": options.temperature,
                "top_p": options.top_p,
                "top_k": options.top_k,
                "num_predict": options.max_tokens,
                "repeat_penalty": options.repetition_penalty
            },
            "stream": options.stream
        }
        
        # Count prompt tokens
        prompt_text = "\n".join([msg.content for msg in messages])
        prompt_tokens = self.token_counter.count_tokens(prompt_text)
        
        # Make request
        try:
            if options.stream:
                full_response = ""
                response = requests.post(
                    f"{self.api_base}/chat",
                    json=request_data,
                    stream=True,
                    timeout=self.timeout
                )
                response.raise_for_status()
                
                for line in response.iter_lines():
                    if not line:
                        continue
                    
                    chunk = json.loads(line)
                    if "error" in chunk:
                        raise RuntimeError(f"Ollama API error: {chunk['error']}")
                    
                    if "message" in chunk:
                        message_content = chunk["message"]["content"]
                        full_response += message_content
                    
                    # Check if generation is done
                    if chunk.get("done", False):
                        break
            else:
                response = requests.post(
                    f"{self.api_base}/chat",
                    json=request_data,
                    timeout=self.timeout
                )
                response.raise_for_status()
                
                data = response.json()
                if "error" in data:
                    raise RuntimeError(f"Ollama API error: {data['error']}")
                
                full_response = data["message"]["content"]
        
        except Exception as e:
            logger.error(f"Error calling Ollama API: {str(e)}")
            raise RuntimeError(f"LLM API error: {str(e)}")
        
        # Count completion tokens
        completion_tokens = self.token_counter.count_tokens(full_response)
        
        # Create usage metrics
        usage = LLMUsage(
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens
        )
        
        return LLMResponse(
            content=full_response.strip(),
            usage=usage,
            model_id=self.model_id
        )
    
    def _generate_local_api(self, messages: List[LLMMessage], options: LLMRequestOptions) -> LLMResponse:
        """Generate using local API (generic implementation)"""
        # Format prompt
        prompt = self.prompt_builder.format_prompt(messages)
        
        # Count tokens
        prompt_tokens = self.token_counter.count_tokens(prompt)
        
        # Prepare request
        request_data = {
            "prompt": prompt,
            "max_tokens": options.max_tokens,
            "temperature": options.temperature,
            "top_p": options.top_p,
            "stop": options.stop_sequences if options.stop_sequences else None,
            "stream": options.stream
        }
        
        # Make request
        try:
            if options.stream:
                full_response = ""
                response = requests.post(
                    f"{self.api_base}/generate",
                    json=request_data,
                    stream=True,
                    timeout=self.timeout
                )
                response.raise_for_status()
                
                for line in response.iter_lines():
                    if not line:
                        continue
                    
                    try:
                        chunk = json.loads(line)
                        text = chunk.get("text", "")
                        full_response += text
                    except json.JSONDecodeError:
                        # Handle non-JSON responses
                        full_response += line.decode('utf-8')
            else:
                response = requests.post(
                    f"{self.api_base}/generate",
                    json=request_data,
                    timeout=self.timeout
                )
                response.raise_for_status()
                
                data = response.json()
                full_response = data.get("text", "")
        
        except Exception as e:
            logger.error(f"Error calling local API: {str(e)}")
            raise RuntimeError(f"LLM API error: {str(e)}")
        
        # Count completion tokens
        completion_tokens = self.token_counter.count_tokens(full_response)
        
        # Create usage metrics
        usage = LLMUsage(
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens
        )
        
        return LLMResponse(
            content=full_response.strip(),
            usage=usage,
            model_id=self.model_id
        )

class LLMModuleManager:
    """Manager for LLM modules and models"""
    
    def __init__(self, cache_dir: Optional[str] = None):
        """
        Initialize the manager
        
        Args:
            cache_dir: Directory to cache models
        """
        self.cache_dir = cache_dir or os.path.expanduser("~/.cache/unravel-ai/models")
        os.makedirs(self.cache_dir, exist_ok=True)
        
        self.active_processes = {}
        self.api_clients = {}
    
    def ensure_llamacpp_server(self, model_path: str, port: int = 8080, n_ctx: int = 2048, 
                               n_threads: Optional[int] = None) -> bool:
        """
        Ensure llama.cpp server is running
        
        Args:
            model_path: Path to the model file
            port: Server port
            n_ctx: Context size
            n_threads: Number of threads to use
            
        Returns:
            Whether the server is running
        """
        server_id = f"llamacpp_{port}"
        
        # Check if already running
        if server_id in self.active_processes:
            process = self.active_processes[server_id]
            if process.poll() is None:  # Process is still running
                return True
            else:
                # Process has terminated
                del self.active_processes[server_id]
        
        # Check if another server is running on this port
        try:
            response = requests.get(f"http://localhost:{port}/health", timeout=2)
            if response.status_code == 200:
                logger.info(f"llama.cpp server already running on port {port}")
                return True
        except:
            # Server not running
            pass
        
        # Find llama-server executable
        server_executable = self._find_llamacpp_executable()
        if not server_executable:
            logger.error("llama-server executable not found, attempting to install...")
            if not self._install_llamacpp():
                logger.error("Failed to install llama.cpp")
                return False
            server_executable = self._find_llamacpp_executable()
            if not server_executable:
                logger.error("Still cannot find llama-server executable")
                return False
        
        # Set number of threads
        if n_threads is None:
            n_threads = max(1, psutil.cpu_count(logical=False) - 1)
        
        # Start server
        logger.info(f"Starting llama.cpp server with model {model_path} on port {port}...")
        cmd = [
            server_executable,
            "-m", model_path,
            "--host", "127.0.0.1",
            "--port", str(port),
            "-c", str(n_ctx),
            "--threads", str(n_threads),
            "--ctx-size", str(n_ctx),
            "--batch-size", "512"
        ]
        
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        
        self.active_processes[server_id] = process
        
        # Wait for server to start
        max_wait = 30  # seconds
        start_time = time.time()
        while time.time() - start_time < max_wait:
            try:
                response = requests.get(f"http://localhost:{port}/health", timeout=2)
                if response.status_code == 200:
                    logger.info(f"llama.cpp server started successfully on port {port}")
                    return True
            except:
                # Server not ready yet
                time.sleep(1)
                
                # Check if process has terminated
                if process.poll() is not None:
                    stdout, stderr = process.communicate()
                    logger.error(f"Server process terminated: {stderr}")
                    del self.active_processes[server_id]
                    return False
        
        logger.error(f"Timed out waiting for llama.cpp server to start on port {port}")
        return False
    
    def _find_llamacpp_executable(self) -> Optional[str]:
        """Find llama-server executable"""
        # Try common names
        for name in ["llama-server", "server"]:
            # Check in PATH
            path = shutil.which(name)
            if path:
                return path
            
            # Check common locations
            for location in [
                os.path.expanduser("~/.local/bin"),
                "/usr/local/bin",
                "/usr/bin",
                os.path.join(os.path.dirname(sys.executable), "Scripts")  # Windows
            ]:
                candidate = os.path.join(location, name)
                if os.path.exists(candidate) and os.access(candidate, os.X_OK):
                    return candidate
        
        return None
    
    def _install_llamacpp(self) -> bool:
        """Install llama.cpp"""
        try:
            # Create build directory
            build_dir = os.path.join(self.cache_dir, "build_llamacpp")
            os.makedirs(build_dir, exist_ok=True)
            
            # Clone repository
            logger.info("Cloning llama.cpp repository...")
            subprocess.run(
                ["git", "clone", "--depth", "1", "https://github.com/ggerganov/llama.cpp.git"],
                cwd=build_dir,
                check=True
            )
            
            # Build
            logger.info("Building llama.cpp...")
            build_path = os.path.join(build_dir, "llama.cpp", "build")
            os.makedirs(build_path, exist_ok=True)
            
            subprocess.run(
                ["cmake", ".."],
                cwd=build_path,
                check=True
            )
            
            n_threads = max(1, os.cpu_count() or 4)
            subprocess.run(
                ["cmake", "--build", ".", "--config", "Release", "-j", str(n_threads)],
                cwd=build_path,
                check=True
            )
            
            # Install
            logger.info("Installing llama.cpp...")
            install_dir = os.path.expanduser("~/.local/bin")
            os.makedirs(install_dir, exist_ok=True)
            
            # Copy binary
            for src, dst in [
                (os.path.join(build_path, "bin", "llama-server"), os.path.join(install_dir, "llama-server")),
                (os.path.join(build_path, "bin", "main"), os.path.join(install_dir, "llama-cpp"))
            ]:
                if os.path.exists(src):
                    shutil.copy2(src, dst)
                    os.chmod(dst, 0o755)  # Make executable
            
            # Create symlink
            server_path = os.path.join(install_dir, "server")
            if not os.path.exists(server_path):
                os.symlink(os.path.join(install_dir, "llama-server"), server_path)
            
            return True
        except Exception as e:
            logger.error(f"Error installing llama.cpp: {e}")
            return False
    
    def download_model(self, model_id: str, model_file: Optional[str] = None) -> Optional[str]:
        """
        Download a model from Hugging Face
        
        Args:
            model_id: Hugging Face model ID
            model_file: Specific model file to download
            
        Returns:
            Path to the downloaded model or None if failed
        """
        if not HAS_HUGGINGFACE_HUB:
            logger.error("huggingface_hub not installed, cannot download model")
            return None
        
        try:
            from huggingface_hub import hf_hub_download
            
            # Create model directory
            model_dir = os.path.join(self.cache_dir, model_id.replace("/", "_"))
            os.makedirs(model_dir, exist_ok=True)
            
            # Download model file
            if model_file:
                logger.info(f"Downloading {model_id}/{model_file}...")
                model_path = hf_hub_download(
                    repo_id=model_id,
                    filename=model_file,
                    local_dir=model_dir
                )
                return model_path
            else:
                # Try to find a GGUF file
                logger.info(f"Searching for GGUF model in {model_id}...")
                from huggingface_hub import list_repo_files
                
                files = list_repo_files(model_id)
                gguf_files = [f for f in files if f.en